{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from scipy.sparse import vstack\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('dataset/train_preprocess.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "colums = df_train.columns.to_list()\n",
    "colums.remove('review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ac',\n",
       " 'air_panas',\n",
       " 'bau',\n",
       " 'general',\n",
       " 'kebersihan',\n",
       " 'linen',\n",
       " 'service',\n",
       " 'sunrise_meal',\n",
       " 'tv',\n",
       " 'wifi']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ac, neutral: 1814, positive: 51, negative: 417, negative_positive: 1, total: 2283\n",
      "Columns: air_panas, neutral: 1922, positive: 26, negative: 335, negative_positive: 0, total: 2283\n",
      "Columns: bau, neutral: 1911, positive: 12, negative: 360, negative_positive: 0, total: 2283\n",
      "Columns: general, neutral: 2023, positive: 230, negative: 30, negative_positive: 0, total: 2283\n",
      "Columns: kebersihan, neutral: 1350, positive: 205, negative: 722, negative_positive: 6, total: 2283\n",
      "Columns: linen, neutral: 1613, positive: 63, negative: 606, negative_positive: 1, total: 2283\n",
      "Columns: service, neutral: 1649, positive: 247, negative: 386, negative_positive: 1, total: 2283\n",
      "Columns: sunrise_meal, neutral: 2108, positive: 75, negative: 100, negative_positive: 0, total: 2283\n",
      "Columns: tv, neutral: 2075, positive: 13, negative: 195, negative_positive: 0, total: 2283\n",
      "Columns: wifi, neutral: 1928, positive: 25, negative: 330, negative_positive: 0, total: 2283\n"
     ]
    }
   ],
   "source": [
    "for col in colums:\n",
    "    count = df_train[col].value_counts()\n",
    "\n",
    "    if not \"neg_pos\" in count:\n",
    "        count['neg_pos'] = 0\n",
    "        \n",
    "    print(\"Columns: {}, neutral: {}, positive: {}, negative: {}, negative_positive: {}, total: {}\".format(col, count['neut'], count['pos'], count['neg'], count['neg_pos'], count.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fasttext(xtrain):\n",
    "    sentences = [word_tokenize(content.lower()) for content in xtrain]\n",
    "    vectorizer = FastText(sentences, vector_size=300, window=3, min_count=1, workers=4, epochs=1000, sg=0, hs=0)\n",
    "    vectorizer.save('model/test.ft')\n",
    "    print('fasttext model saved at model/test.ft')\n",
    "\n",
    "def norm_sent_vector(sentence, wv):\n",
    "    vecs = [wv[word.lower()] for word in word_tokenize(sentence)] \n",
    "    norm_vecs = [vec / np.linalg.norm(vecs) for vec in vecs if np.linalg.norm(vecs) > 0]\n",
    "    sent_vec = np.mean(norm_vecs, axis=0)\n",
    "    return sent_vec\n",
    "\n",
    "def hyperparam_tuning(xtrain, ytrain, xvalid, yvalid, classifier, param_grid):\n",
    "    # combine train and valid\n",
    "    x = vstack([xtrain, xvalid])\n",
    "    y = ytrain + yvalid\n",
    "    \n",
    "    # create predefined split\n",
    "    # -1 for all training and 0 for all validation\n",
    "    ps = PredefinedSplit([-1] * len(ytrain) + [0] * len(yvalid))\n",
    "    clf = GridSearchCV(classifier, param_grid, cv = ps)\n",
    "    clf = clf.fit(x, y)\n",
    "    return clf\n",
    "\n",
    "def train_and_test(data_train, data_valid, data_test, feature=\"bow\", classifier=\"nb\", save_path=None, ft_path=\"model/test.ft\"):\n",
    "    xtrain = data_train['review']\n",
    "    xvalid = data_valid['review']\n",
    "    xtest = data_test['review']\n",
    "\n",
    "    colums = data_train.columns.to_list()\n",
    "    colums.remove('review')\n",
    "\n",
    "    if feature == \"bow\":\n",
    "        vectorizer = CountVectorizer()\n",
    "    elif feature == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    elif feature == \"fasttext\":\n",
    "        vectorizer = FastText.load(ft_path).wv\n",
    "    else:\n",
    "        raise Exception('Feature unknown. Use \"bow\" or \"tfidf\" or \"fasttext\"')\n",
    "\n",
    "    # transform\n",
    "    if feature == \"bow\" or feature == \"tfidf\":\n",
    "        vectorizer.fit(xtrain)\n",
    "        xtrain = vectorizer.transform(xtrain)\n",
    "        xvalid = vectorizer.transform(xvalid)\n",
    "        xtest = vectorizer.transform(xtest)\n",
    "    elif feature == \"fasttext\":\n",
    "        scaler = MinMaxScaler()\n",
    "        xtrain = scaler.fit_transform([norm_sent_vector(s, vectorizer) for s in xtrain])\n",
    "        xvalid = scaler.fit_transform([norm_sent_vector(s, vectorizer) for s in xvalid])\n",
    "        xtest = scaler.fit_transform([norm_sent_vector(s, vectorizer) for s in xtest])\n",
    "\n",
    "    # all classifiers\n",
    "    classifier_model = {\"nb\" : MultinomialNB(),\n",
    "                        \"svm\": SVC(),\n",
    "                        \"lr\" : LogisticRegression(),\n",
    "                    }\n",
    "    # all params for grid-search\n",
    "    param_grids = {\"nb\" : {\"alpha\": np.linspace(0.001,1,50)},\n",
    "                \"svm\": {'C': [0.01, 0.1, 1, 10, 100], 'kernel': ['rbf', 'linear']},\n",
    "                \"lr\" : {'C': np.linspace(0.001,10,100)},\n",
    "                }\n",
    "\n",
    "    categorical = {}\n",
    "    average_acc = 0\n",
    "    for col in colums:\n",
    "        ytrain = list(data_train[col])\n",
    "        yvalid = list(data_valid[col])\n",
    "        ytest = list(data_test[col])\n",
    "\n",
    "        clf = hyperparam_tuning(xtrain, ytrain, xvalid, yvalid,\n",
    "                                classifier=classifier_model[classifier],\n",
    "                                param_grid=param_grids[classifier])\n",
    "\n",
    "        if feature == \"bow\" or feature == \"tfidf\":\n",
    "            pred = clf.predict(xtest.toarray())\n",
    "        else:\n",
    "            pred = clf.predict(xtest)\n",
    "\n",
    "        f1 = f1_score(ytest, pred, average='macro')\n",
    "        acc = accuracy_score(ytest, pred) \n",
    "        average_acc += acc\n",
    "\n",
    "        categorical[col] = {'f1': f1, 'acc': acc}\n",
    "        \n",
    "        if save_path is not None:\n",
    "            filename = save_path+'/'+feature+'/'+col\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            with open(filename, 'wb') as fout:\n",
    "                pickle.dump((vectorizer, clf), fout)\n",
    "\n",
    "    average_acc = average_acc / len(colums)\n",
    "    return average_acc, categorical\n",
    "    # return f1score, accuracy, clf, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model_path, feature='bow'):\n",
    "\n",
    "    colums = ['ac', 'air_panas', 'bau', 'general', 'kebersihan', 'linen', 'service', 'sunrise_meal', 'tv', 'wifi']\n",
    "    pred = {}\n",
    "    for col in colums:\n",
    "        with open(model_path+'/'+feature+'/'+col, 'rb') as f:\n",
    "            vectorizer, clf = pickle.load(f)\n",
    "\n",
    "            if feature == \"bow\" or feature == \"tfidf\":\n",
    "                x = vectorizer.transform([text])\n",
    "                pred[col] = clf.predict(x.toarray())[0]\n",
    "            elif feature == \"fasttext\":\n",
    "                scaler = MinMaxScaler()\n",
    "                x = scaler.fit_transform([norm_sent_vector(s, vectorizer) for s in [text]])\n",
    "                pred[col] = clf.predict(x)[0]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"dataset/train_preprocess.csv\")\n",
    "data_valid = pd.read_csv(\"dataset/valid_preprocess.csv\")\n",
    "data_test = pd.read_csv(\"dataset/test_preprocess.csv\")\n",
    "acc, categorical = train_and_test(data_train, data_valid, data_test, feature=\"tfidf\", save_path=\"model/train1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ac': {'f1': 0.36887159533073927, 'acc': 0.8321678321678322},\n",
       " 'air_panas': {'f1': 0.3075957313245449, 'acc': 0.8566433566433567},\n",
       " 'bau': {'f1': 0.3069182389937107, 'acc': 0.8531468531468531},\n",
       " 'general': {'f1': 0.3062381852551985, 'acc': 0.8496503496503497},\n",
       " 'kebersihan': {'f1': 0.27294255621461916, 'acc': 0.5699300699300699},\n",
       " 'linen': {'f1': 0.27191166321601107, 'acc': 0.6888111888111889},\n",
       " 'service': {'f1': 0.27991886409736305, 'acc': 0.7237762237762237},\n",
       " 'sunrise_meal': {'f1': 0.32, 'acc': 0.9230769230769231},\n",
       " 'tv': {'f1': 0.31553100061387357, 'acc': 0.8986013986013986},\n",
       " 'wifi': {'f1': 0.3109452736318408, 'acc': 0.8741258741258742}}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ac': 'neut',\n",
       " 'air_panas': 'neut',\n",
       " 'bau': 'neut',\n",
       " 'general': 'neut',\n",
       " 'kebersihan': 'neg',\n",
       " 'linen': 'neut',\n",
       " 'service': 'neut',\n",
       " 'sunrise_meal': 'neut',\n",
       " 'tv': 'neut',\n",
       " 'wifi': 'neut'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"lumayan nyaman,tp kebersihan kmr mandi perlu ditingkatkan lg biar gk ada kuning2 di sudutnya lbh bgs\", model_path=\"model/train1\", feature=\"tfidf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1d81e0ab65a2d871dd04cd5480301015f0912bc0455ad9e82832e105402504a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
